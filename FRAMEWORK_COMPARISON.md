# Framework Comparison Tool

## Overview

The Framework Comparison Tool was an MCP (Model Context Protocol) tool that allowed simultaneous story generation across all three AI frameworks (Semantic Kernel, LangChain, and LangGraph) for direct comparison of results, costs, and performance metrics.

## Purpose

This tool was designed to:
- Compare the quality of stories generated by different frameworks
- Analyze cost differences between frameworks
- Measure performance metrics (generation time, token usage)
- Identify the most suitable framework for specific use cases

## How It Worked

### Concurrent Execution
The tool executed all three framework services concurrently using Python's `asyncio.gather()`, significantly reducing total execution time compared to sequential processing.

### Data Collection
For each framework, the tool collected:
- **Story Output**: The generated story text
- **Cost Metrics**: Estimated cost in USD based on token usage
- **Token Usage**: Total tokens consumed (input + output)
- **Generation Time**: Time taken in milliseconds
- **Model Information**: Which AI model was used
- **Success/Failure Status**: Whether the generation succeeded or failed

### Comparison Analysis
The tool provided comprehensive analysis including:
- **Total Metrics**: Combined cost and token usage across all frameworks
- **Averages**: Average cost, tokens, and story length
- **Winners**: Identification of:
  - Fastest framework (lowest generation time)
  - Most economical framework (lowest cost)
  - Most verbose framework (longest story)

## Implementation Details

```python
@mcp.tool()
async def compare_frameworks(
    primary_character: str,
    secondary_character: str
) -> Dict[str, Any]:
    """Generate stories using all three frameworks and compare results, costs, and performance."""
    
    # Initialize all three services
    semantic_service = SemanticKernelService()
    langchain_service = LangChainService()
    langgraph_service = LangGraphService()
    
    # Generate stories concurrently
    tasks = [
        semantic_service.generate_story(primary_character, secondary_character),
        langchain_service.generate_story(primary_character, secondary_character),
        langgraph_service.generate_story(primary_character, secondary_character)
    ]
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Process and analyze results...
```

## Response Format

The tool returned a structured response containing:

```json
{
    "primary_character": "Hero",
    "secondary_character": "Dragon",
    "total_execution_time_ms": 3456.78,
    "frameworks": {
        "semantic_kernel": {
            "success": true,
            "story": "Once upon a time...",
            "cost": 0.0045,
            "tokens": 450,
            "generation_time_ms": 1234.56,
            "story_length": 2500,
            "model": "gpt-4"
        },
        "langchain": { ... },
        "langgraph": { ... }
    },
    "comparison": {
        "total_cost": 0.0135,
        "total_tokens": 1350,
        "successful_generations": 3,
        "failed_generations": 0,
        "averages": {
            "cost": 0.0045,
            "tokens": 450,
            "story_length": 2600
        },
        "winners": {
            "fastest": "semantic_kernel",
            "cheapest": "langchain",
            "longest_story": "langgraph"
        }
    }
}
```

## Use Cases

### A/B Testing
- Compare different frameworks for the same input to determine which produces better results
- Validate consistency across frameworks
- Identify framework-specific strengths and weaknesses

### Cost Optimization
- Determine the most cost-effective framework for production use
- Balance between quality and cost
- Budget planning for AI operations

### Performance Benchmarking
- Measure response times under different loads
- Identify bottlenecks in specific frameworks
- Optimize for latency-sensitive applications

### Quality Assessment
- Compare narrative styles between frameworks
- Evaluate coherence and creativity
- Determine which framework best suits specific content types

## Logging and Monitoring

The tool included comprehensive logging at each stage:
- Service initialization
- Concurrent task launching
- Individual framework results
- Error handling for failed generations
- Final comparison metrics

All logs used structured logging with the prefix "MCP:" for easy filtering and analysis.

## Error Handling

The tool was designed to be resilient:
- Used `return_exceptions=True` in `asyncio.gather()` to capture failures without stopping other tasks
- Continued processing even if one or more frameworks failed
- Provided detailed error information for failed frameworks
- Calculated metrics only from successful generations

## Future Enhancements

While the tool has been removed from the MCP interface, the concept could be extended with:
- Historical comparison tracking
- Statistical analysis over multiple runs
- Custom evaluation metrics
- Integration with quality assessment tools
- Automated framework selection based on requirements
- Cost prediction models
- Performance optimization recommendations

## Alternative Implementation

Without MCP, similar functionality can be achieved through:
1. A dedicated API endpoint for framework comparison
2. A command-line tool for batch testing
3. Integration into the existing web interface
4. Scheduled comparison jobs for monitoring

The core comparison logic remains valuable for understanding the trade-offs between different AI frameworks and optimizing the platform's performance and cost efficiency.