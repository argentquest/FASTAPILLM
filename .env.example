# AI Content Generation Platform - Environment Configuration
# 
# This configuration uses an OpenAI-compatible provider.
# Copy this file to .env and configure your settings.

# =============================================================================
# PROVIDER CONFIGURATION
# =============================================================================
# Your API key for authentication
PROVIDER_API_KEY=your_api_key_here

# Base URL for the API (must be OpenAI-compatible)
# Examples:
# - Ollama: http://localhost:11434/v1
# - LM Studio: http://localhost:1234/v1
# - vLLM: http://localhost:8000/v1
# - Tachyon: https://api.tachyon.ai/v1
PROVIDER_API_BASE_URL=https://api.your-provider.com/v1

# Model name/identifier
# Examples: llama2, mistral, gpt-3.5-turbo, etc.
PROVIDER_MODEL=your-model-name

# Display name for your provider
PROVIDER_NAME=Your Provider

# API compatibility type (currently only "openai" is supported)
PROVIDER_API_TYPE=openai

# Additional HTTP headers (JSON format)
# Examples:
# - Bearer token: {"Authorization": "Bearer your_token"}
# - Custom header: {"X-API-Key": "your_key"}
PROVIDER_HEADERS={}

# =============================================================================
# APPLICATION SETTINGS
# =============================================================================
DEBUG_MODE=false
LOG_LEVEL=INFO
DATABASE_URL=sqlite:///./stories.db

# =============================================================================
# API SETTINGS
# =============================================================================
OPENAI_TIMEOUT=120
MAX_TOKENS=1000
TEMPERATURE=0.7

# =============================================================================
# SECURITY SETTINGS
# =============================================================================
CORS_ORIGINS=["http://localhost:8000","http://127.0.0.1:8000"]
MAX_REQUEST_SIZE=1048576

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
LOG_FILE_PATH=logs/app.log
LOG_ROTATION_HOURS=1
LOG_RETENTION_DAYS=7

# =============================================================================
# RETRY CONFIGURATION
# =============================================================================
# Enable/disable automatic retry mechanisms
RETRY_ENABLED=true

# Maximum number of retry attempts for failed operations
RETRY_MAX_ATTEMPTS=3

# Maximum wait time between retries (seconds)
RETRY_MAX_WAIT_SECONDS=30

# Exponential backoff multiplier
RETRY_MULTIPLIER=2.0

# Minimum wait time between retries (seconds)
RETRY_MIN_WAIT_SECONDS=1.0

# =============================================================================
# RATE LIMITING CONFIGURATION
# =============================================================================
# Enable/disable rate limiting globally
RATE_LIMITING_ENABLED=true

# Per IP address limits (requests per minute)
RATE_LIMIT_PER_IP=60

# Per endpoint limits (requests per minute per IP)
# Story generation endpoints (most expensive operations)
RATE_LIMIT_STORY_GENERATION=15

# List and query endpoints (moderate database operations)  
RATE_LIMIT_LIST_ENDPOINTS=30

# Health and status endpoints (lightweight operations)
RATE_LIMIT_HEALTH_STATUS=100

# Global server limits (requests per minute across all users)
RATE_LIMIT_GLOBAL_SERVER=1000

# Rate limiting behavior settings
RATE_LIMIT_STORAGE_BACKEND=memory
RATE_LIMIT_TIME_WINDOW=fixed
RATE_LIMIT_RETURN_STATUS_CODE=429

# =============================================================================
# QUICK START GUIDE
# =============================================================================
# 1. Copy this file to .env:
#    cp .env.example .env
#
# 2. Configure your provider:
#    - Set PROVIDER_API_KEY to your API key
#    - Set PROVIDER_API_BASE_URL to your provider's endpoint
#    - Set PROVIDER_MODEL to your chosen model
#    - Update PROVIDER_NAME to describe your provider
#
# 3. Optional: Adjust rate limits and retry settings based on your needs
#    - Increase RATE_LIMIT_STORY_GENERATION for higher throughput
#    - Adjust RETRY_MAX_ATTEMPTS for different failure tolerance
#    - Set RATE_LIMITING_ENABLED=false to disable rate limiting
#
# 4. Start the application:
#    python backend/main.py
#
# 5. Test rate limiting (optional):
#    python test_rate_limiting.py
#
# For more details on supported providers, see PROVIDERS.md