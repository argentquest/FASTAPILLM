# Custom/Third-Party Provider Configuration Template
# Copy this file to .env and fill in your custom provider details

# =============================================================================
# AI PROVIDER CONFIGURATION
# =============================================================================
LLM_PROVIDER=custom

# =============================================================================
# CUSTOM PROVIDER SETTINGS (Required)
# =============================================================================
# Your API key for the custom provider
CUSTOM_API_KEY=your_custom_api_key_here

# Base URL for the API (must be OpenAI-compatible)
# Examples:
# - Tachyon: https://api.tachyon.ai/v1
# - Ollama: http://localhost:11434/v1
# - LM Studio: http://localhost:1234/v1
# - vLLM: http://localhost:8000/v1
# - FastChat: http://localhost:8000/v1
# - LocalAI: http://localhost:8080/v1
# - Text Generation Inference: http://localhost:3000/v1
CUSTOM_API_BASE_URL=https://api.your-provider.com/v1

# Model name/identifier to use
# This depends on your provider's available models
# Examples:
# - Tachyon: tachyon-fast, tachyon-balanced, tachyon-creative
# - Ollama: llama2, codellama, mistral, neural-chat
# - Local models: gpt-3.5-turbo, llama-2-7b-chat, etc.
CUSTOM_MODEL=your-model-name

# Display name for your provider (used in logs and UI)
CUSTOM_PROVIDER_NAME=Your Custom Provider

# API type (currently only 'openai' is supported)
CUSTOM_API_TYPE=openai

# Additional HTTP headers as JSON (optional)
# Example: {"Authorization": "Bearer token", "X-Custom-Header": "value"}
CUSTOM_HEADERS={}

# =============================================================================
# APPLICATION SETTINGS
# =============================================================================
# Enable debug mode for development (shows detailed logs and API docs)
DEBUG_MODE=false

# Logging level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Database connection string
# SQLite for development (default)
DATABASE_URL=sqlite:///./stories.db
# PostgreSQL for production (uncomment and configure if needed)
# DATABASE_URL=postgresql://username:password@localhost:5432/ai_content_platform

# =============================================================================
# API SETTINGS
# =============================================================================
# Timeout for API calls in seconds
OPENAI_TIMEOUT=120

# Maximum tokens to generate per request
MAX_TOKENS=1000

# Temperature for response randomness (0.0-2.0, lower = more deterministic)
TEMPERATURE=0.7

# =============================================================================
# SECURITY SETTINGS
# =============================================================================
# Allowed CORS origins (comma-separated URLs)
CORS_ORIGINS=["http://localhost:8000","http://127.0.0.1:8000"]

# Maximum request size in bytes (1MB default)
MAX_REQUEST_SIZE=1048576

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
# Path to log file
LOG_FILE_PATH=logs/app.log

# Hours between log file rotation
LOG_ROTATION_HOURS=1

# Days to retain old log files
LOG_RETENTION_DAYS=7

# =============================================================================
# SUPPORTED CUSTOM PROVIDERS
# =============================================================================

# 1. TACHYON LLM (High-performance commercial service)
# CUSTOM_API_BASE_URL=https://api.tachyon.ai/v1
# CUSTOM_MODEL=tachyon-fast
# CUSTOM_PROVIDER_NAME=Tachyon LLM

# 2. OLLAMA (Local LLM runner)
# CUSTOM_API_BASE_URL=http://localhost:11434/v1
# CUSTOM_MODEL=llama2
# CUSTOM_PROVIDER_NAME=Ollama Local

# 3. LM STUDIO (Local LLM server)
# CUSTOM_API_BASE_URL=http://localhost:1234/v1
# CUSTOM_MODEL=local-model
# CUSTOM_PROVIDER_NAME=LM Studio

# 4. vLLM (High-throughput inference server)
# CUSTOM_API_BASE_URL=http://localhost:8000/v1
# CUSTOM_MODEL=meta-llama/Llama-2-7b-chat-hf
# CUSTOM_PROVIDER_NAME=vLLM Server

# 5. FASTCHAT (Multi-model serving)
# CUSTOM_API_BASE_URL=http://localhost:8000/v1
# CUSTOM_MODEL=vicuna-7b-v1.5
# CUSTOM_PROVIDER_NAME=FastChat

# 6. LOCALAI (OpenAI compatible local API)
# CUSTOM_API_BASE_URL=http://localhost:8080/v1
# CUSTOM_MODEL=gpt-3.5-turbo
# CUSTOM_PROVIDER_NAME=LocalAI

# 7. TEXT GENERATION INFERENCE (Hugging Face)
# CUSTOM_API_BASE_URL=http://localhost:3000/v1
# CUSTOM_MODEL=bigscience/bloom
# CUSTOM_PROVIDER_NAME=Text Generation Inference

# =============================================================================
# PROVIDER-SPECIFIC SETUP GUIDES
# =============================================================================

# TACHYON SETUP:
# 1. Sign up at https://tachyon.ai
# 2. Get API key from dashboard
# 3. Choose model: tachyon-fast, tachyon-balanced, tachyon-creative
# 4. Use: https://api.tachyon.ai/v1 as base URL

# OLLAMA SETUP:
# 1. Install Ollama: https://ollama.ai
# 2. Pull a model: ollama pull llama2
# 3. Start server: ollama serve
# 4. Use: http://localhost:11434/v1 as base URL

# LM STUDIO SETUP:
# 1. Install LM Studio: https://lmstudio.ai
# 2. Download a model through the UI
# 3. Start local server in LM Studio
# 4. Use: http://localhost:1234/v1 as base URL

# vLLM SETUP:
# 1. Install vLLM: pip install vllm
# 2. Start server: python -m vllm.entrypoints.openai.api_server --model MODEL_NAME
# 3. Use: http://localhost:8000/v1 as base URL

# =============================================================================
# CUSTOM PROVIDER REQUIREMENTS
# =============================================================================
# Your custom provider must:
# 1. Be OpenAI API compatible (same endpoints and format)
# 2. Support the /v1/chat/completions endpoint
# 3. Accept JSON requests with messages array
# 4. Return responses in OpenAI format with choices array
# 5. Include usage information (tokens) in responses

# =============================================================================
# TROUBLESHOOTING
# =============================================================================
# Common issues:
# 1. Connection refused: Check if provider server is running
# 2. 404 errors: Verify the base URL and endpoint paths
# 3. Authentication errors: Check API key and headers
# 4. Model not found: Verify model name is correct for your provider
# 5. Timeout errors: Increase OPENAI_TIMEOUT for slower providers

# =============================================================================
# AUTHENTICATION METHODS
# =============================================================================
# Most providers use one of these authentication methods:

# 1. Bearer token in Authorization header (most common):
# CUSTOM_HEADERS={"Authorization": "Bearer your_api_key"}

# 2. API key in custom header:
# CUSTOM_HEADERS={"X-API-Key": "your_api_key"}

# 3. API key as query parameter (add to base URL):
# CUSTOM_API_BASE_URL=https://api.provider.com/v1?api_key=your_key

# 4. Basic authentication (rare):
# CUSTOM_HEADERS={"Authorization": "Basic base64_encoded_credentials"}

# =============================================================================
# GETTING STARTED
# =============================================================================
# 1. Choose your custom provider from the list above
# 2. Follow the provider-specific setup guide
# 3. Configure the settings in this file
# 4. Test the connection with a simple request
# 5. Save as .env and start the application